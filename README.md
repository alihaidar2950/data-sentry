# ğŸ”¥ Data Sentry

> **High-Performance Business Data Scraper with Real-Time Google Sheets Automation**

A production-grade data automation system that continuously monitors websites, extracts structured business data at scale, cleans it, stores it, syncs it to Google Sheets, and triggers intelligent alerts when critical changes occur.

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Code Quality](https://img.shields.io/badge/Code%20Quality-Production-brightgreen.svg)]()

---

## ğŸ¯ Why Data Sentry?

Unlike basic web scrapers, **Data Sentry** is engineered for real business needs:

- âœ… **E-commerce sellers** tracking competitor pricing
- âœ… **Real estate investors** monitoring new listings
- âœ… **Price arbitrage traders** detecting opportunities
- âœ… **Marketing teams** gathering market intelligence
- âœ… **Local businesses** automating supplier price updates

This isn't a toy scraper â€” it's a **backend automation service** built with senior-level engineering practices.

---

## âš¡ Core Features

### ğŸš€ **Async High-Speed Web Scraping**
- Concurrent scraping of hundreds of pages using `asyncio` and `aiohttp`
- Smart retry logic with exponential backoff
- Configurable rate limiting to prevent bans
- User-defined CSS/XPath selectors
- Customizable crawl depth

### ğŸ§¹ **Intelligent Data Cleaning & Normalization**
- Currency normalization across formats
- Missing field handling
- Duplicate detection and removal
- Robust HTML parsing and cleanup
- Type conversion and validation

### ğŸ’¾ **Flexible Storage Options**
- CSV export for quick analysis
- SQLite database for historical tracking
- Data versioning and diff comparison
- Query interface for historical data

### ğŸ“Š **Google Sheets Integration**
- OAuth2 authenticated API access
- Auto-create and manage sheets
- Real-time data synchronization
- Bulk insert and update operations
- Optional conditional formatting

### ğŸ”” **Smart Change Detection & Alerts**
- Price change monitoring
- Stock status tracking
- New listing detection
- Removed product alerts
- Multi-channel notifications (Email, Slack, Discord)

---

## ğŸ”¥ Advanced Features

### ğŸŒ **Proxy Rotation System**
- Support for free and premium proxy services
- Automatic rotation to prevent IP bans
- User-agent randomization
- Session management

### ğŸ” **Authenticated Scraping**
- Login to member-only sites
- Cookie persistence
- Session handling
- Headless browser fallback (Playwright/Selenium)

### â° **Built-in Scheduler**
- Cron-style job scheduling
- Intervals: 5 min, hourly, daily, weekly
- Background task execution
- Parallel job management

### ğŸŒ **REST API Control Layer** *(Optional)*
- FastAPI-powered endpoints
- Start/stop scraping jobs
- Query data exports
- Configure alerts
- Real-time status monitoring

### ğŸ³ **Dockerized Deployment**
- Production-ready `Dockerfile`
- `docker-compose.yml` for one-command startup
- Environment variable configuration
- Volume mounting for data persistence

---

## ğŸ—ï¸ Project Architecture

```
data-sentry/
â”œâ”€â”€ scraper/              # Core scraping engine
â”‚   â”œâ”€â”€ fetcher.py        # Async HTTP fetching
â”‚   â”œâ”€â”€ parser.py         # HTML/JSON parsing
â”‚   â””â”€â”€ normalizer.py     # Data cleaning & validation
â”œâ”€â”€ storage/              # Data persistence layer
â”‚   â”œâ”€â”€ csv_store.py      # CSV export functionality
â”‚   â””â”€â”€ db_store.py       # SQLite database operations
â”œâ”€â”€ sheets/               # Google Sheets integration
â”‚   â””â”€â”€ sync.py           # OAuth2 + API sync logic
â”œâ”€â”€ alerts/               # Notification system
â”‚   â”œâ”€â”€ email.py          # SMTP email alerts
â”‚   â”œâ”€â”€ slack.py          # Slack webhook integration
â”‚   â””â”€â”€ discord.py        # Discord webhook integration
â”œâ”€â”€ api/                  # REST API (optional)
â”‚   â””â”€â”€ app.py            # FastAPI application
â”œâ”€â”€ scheduler/            # Job scheduling
â”‚   â””â”€â”€ jobs.py           # Cron-style task runner
â”œâ”€â”€ docker/               # Containerization
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ tests/                # Unit & integration tests
â”œâ”€â”€ config/               # Configuration files
â”œâ”€â”€ logs/                 # Application logs
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- Google Cloud account (for Sheets API)
- Docker (optional, for containerized deployment)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/alihaidar2950/data-sentry.git
cd data-sentry
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure Google Sheets API**
- Create a project in [Google Cloud Console](https://console.cloud.google.com/)
- Enable Google Sheets API
- Download OAuth2 credentials JSON
- Place in `config/credentials.json`

5. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env with your configuration
```

6. **Run your first scrape**
```bash
python main.py --url https://example.com --selector ".product"
```

---

## ğŸ“– Usage Examples

### Basic Scraping
```python
from scraper.fetcher import AsyncFetcher
from scraper.parser import HTMLParser

# Initialize scraper
fetcher = AsyncFetcher(max_concurrent=50)
parser = HTMLParser()

# Scrape data
urls = ["https://example.com/page1", "https://example.com/page2"]
raw_data = await fetcher.fetch_all(urls)
clean_data = parser.parse(raw_data, selector=".product")
```

### Google Sheets Sync
```python
from sheets.sync import SheetsSync

# Initialize sync
sync = SheetsSync(credentials_path="config/credentials.json")

# Create and populate sheet
sheet_id = sync.create_sheet("Product Prices")
sync.append_rows(sheet_id, clean_data)
```

### Alert Configuration
```python
from alerts.email import EmailAlert
from alerts.slack import SlackAlert

# Configure alerts
email = EmailAlert(smtp_config)
slack = SlackAlert(webhook_url)

# Send notifications
if price_changed:
    email.send("Price Alert", f"Price dropped to ${new_price}")
    slack.send(f"ğŸš¨ Price alert: ${new_price}")
```

---

## ğŸ³ Docker Deployment

```bash
# Build image
docker-compose build

# Run service
docker-compose up -d

# View logs
docker-compose logs -f

# Stop service
docker-compose down
```

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=scraper --cov-report=html

# Run specific test file
pytest tests/test_fetcher.py
```

---

## ğŸ“Š Real-World Use Cases

### ğŸ“¦ **E-commerce Price Monitoring**
**Scenario**: Shopify seller tracks 5 competitors  
**Frequency**: Every 2 hours  
**Output**: Google Sheet with price changes highlighted  
**Value**: $500â€“$2,000

### ğŸ  **Real Estate Deal Finder**
**Scenario**: Investor monitors new listings  
**Frequency**: Every 10 minutes  
**Alerts**: Instant Slack notification  
**Value**: $1,000+

### ğŸ’¼ **Supplier Price Tracker**
**Scenario**: Local business tracks supplier pricing  
**Frequency**: Daily  
**Output**: Auto-updated reporting dashboard  
**Value**: $300â€“$800/month (recurring)

---

## ğŸ’° Commercial Applications

This project is perfect for:

| Service Offering                 | Typical Price Range |
|----------------------------------|---------------------|
| Single scraper â†’ CSV export      | $150â€“$300           |
| Scraper + Sheets + alerts        | $400â€“$900           |
| Full deployed automation service | $1,000â€“$2,500       |
| Monthly monitoring contract      | $200â€“$800/month     |

---

## ğŸ› ï¸ Tech Stack

- **Core**: Python 3.9+, asyncio, aiohttp
- **Parsing**: BeautifulSoup4, lxml
- **Data**: pandas, SQLite
- **API**: FastAPI, Pydantic
- **Scheduling**: APScheduler
- **Cloud**: Google Sheets API, OAuth2
- **Alerts**: SMTP, Slack/Discord webhooks
- **Containerization**: Docker, docker-compose
- **Testing**: pytest, pytest-cov
- **Code Quality**: Black, flake8, mypy

---

## ğŸ¯ Skills Demonstrated

This project showcases:

âœ… **Async Python Programming** (asyncio, aiohttp)  
âœ… **Concurrent Processing** (multithreading, parallel execution)  
âœ… **API Integration** (Google Sheets, REST APIs)  
âœ… **Data Engineering** (ETL pipelines, normalization)  
âœ… **Backend Development** (FastAPI, service architecture)  
âœ… **DevOps** (Docker, CI/CD, automation)  
âœ… **Software Quality** (testing, static analysis, logging)  
âœ… **Production Systems** (error handling, monitoring, alerts)

---

## ğŸ—ºï¸ Roadmap

- [ ] **Phase 1**: Core scraping engine + CSV export
- [ ] **Phase 2**: Google Sheets integration
- [ ] **Phase 3**: Alert system (email, Slack, Discord)
- [ ] **Phase 4**: Proxy rotation + anti-ban measures
- [ ] **Phase 5**: REST API layer
- [ ] **Phase 6**: Scheduler + background jobs
- [ ] **Phase 7**: Docker deployment
- [ ] **Phase 8**: Web UI dashboard (React/Vue)
- [ ] **Phase 9**: Cloud deployment (AWS/GCP/Azure)
- [ ] **Phase 10**: SaaS multi-tenant version

---

## ğŸ“ Configuration

Create a `.env` file in the root directory:

```env
# Scraper Settings
MAX_CONCURRENT_REQUESTS=50
REQUEST_TIMEOUT=30
RETRY_ATTEMPTS=3
RATE_LIMIT_DELAY=1

# Google Sheets
GOOGLE_CREDENTIALS_PATH=config/credentials.json
DEFAULT_SHEET_NAME=Scraped Data

# Database
DATABASE_PATH=data/scraper.db

# Alerts
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=your-email@gmail.com
SMTP_PASSWORD=your-app-password
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
DISCORD_WEBHOOK_URL=https://discord.com/api/webhooks/YOUR/WEBHOOK

# Scheduler
SCRAPE_INTERVAL=3600  # seconds
ENABLE_SCHEDULER=true

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/scraper.log
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Ali Haidar**
- GitHub: [@alihaidar2950](https://github.com/alihaidar2950)
- Email: alihaidar2950@gmail.com

---

## ğŸŒŸ Acknowledgments

Built with modern Python best practices and production-grade engineering standards. Perfect for:
- Backend Engineer roles
- Data Engineer positions
- Python Automation Engineer jobs
- Freelance scraping projects
- Startup automation contracts

---

## ğŸ“ Support & Services

Looking for custom scraping solutions or automation services?

**Available for:**
- Custom web scraping projects
- Data automation consulting
- API integration services
- Backend system development

**Contact:** alihaidar2950@gmail.com

---

<div align="center">

**â­ Star this repo if you find it useful!**

Made with â¤ï¸ by Ali Haidar

</div>
